# Beyond Marginal Distributions: A Framework to Evaluate the Representativeness of Demographic-Aligned LLMs

This repository accompanies the paper:

> **Beyond Marginal Distributions: A Framework to Evaluate the Representativeness of Demographic-Aligned LLMs**  
> *Tristan Williams, Franziska Weeber, Sebastian Padó, and Alan Akbik*

## Overview

Large language models (LLMs) are increasingly used as proxies for human opinions, values, and beliefs. A growing body of work studies *demographic alignment* or *steerability* by comparing model-generated survey responses to human data. Existing evaluations, however, focus almost exclusively on matching **marginal response distributions**, treating survey items as independent.

This repository implements a framework for evaluating ***representativeness beyond marginals***, motivated by cultural values research. In addition to marginal distributions, we assess whether aligned models reproduce the **multivariate correlation structure** observed in real populations.

We illustrate the framework by using data from the ***World Values Survey (WVS)*** and demonstrate that models which appear well-aligned under marginal-based evaluations can nonetheless fail to capture the latent structure of human values. 


## Key Contributions

### Evaluation Framework

The framework evaluates representativeness from two perspectives:

1. Marginal response distributions: How well do simulated responses approximate the ground truth response distribution? 
2. Multivariate correlation structures: Do the simulated responses reproduce the interactions between questions and topics that underpin the WVS and related social scientific research?

### Model comparison

We then apply our framework to compare two techniques for model steering using World Values Survey data:
1. Demographic fine-tuning (specifically OpinionGPT)
2. Persona prompting (using Phi 3 Mini)

Additionally, we consider an unsteered version of Phi 3 as a baseline. We find that OpinionGPT displays 
improved marginal distribution similarity and response diversity compared to the persona prompting approach and the unsteered baseline.
However, neither model steering approach accurately reproduces the correlation structures observed in the true human survey responses.

## Data

- Human responses are drawn from the **World Values Survey** Wave 7.
- Model responses are generated by prompting or fine-tuned models to answer the same survey items under controlled conditions.

## Repository Structure

```text
├── data_files
│   └── variables           # WVS survey items
├── experiments             # YAML configuration files for experiments
├── jobs                    # Orchestration scripts for simulations and evaluations (see below)
├── scripts                 # Python entry points
├── src
│   ├── analysis            # Representativeness metrics and evaluation logic
│   ├── data                # Data loading and preprocessing utilities
│   ├── demographics        # Demographic grouping and conditioning logic
│   ├── prompting           # Prompt construction for model simulations
│   └── simulation          # Model response generation and sampling
└── tests                   # unit and data integrity tests
```

## Setup
1. Install dependencies in your virtual environment with `pip install -r requirements.txt`
2. Use the `.env.example` to create an `.env` file with your HF token
3. Download the file `WVS Cross-National Wave 7 csv v6 0.zip` from the WVS website [here](https://www.worldvaluessurvey.org/WVSDocumentationWV7.jsp) and, after unzipping, place the csv file in `data_files/WV7`

## Usage
1. Create a new experiment configuration `experiments/<your_experiment>.yaml`. Our configuration is defined in `base.yaml` (see below), all experiments will load this first and then overwrite any parameters specified in your experiment config (see `test.yaml` for an example)


2. Simulate responses with all models (we used an NVIDIA A100 80GB PCIe GPU)
```
./jobs/run_simulation.sh <your_experiment>
```
3. Apply the evaluation framework
```
./jobs/run_evaluation.sh <your_experiment>
```

## Our Experiment Configuration

```yaml
setup:
  name: "base"
  random_seed: 42

files:
  directory: "data_files"
  variables: "variables.csv"
  subset: "final_subset.json"

simulation:
  sample_size: 500
  batch_size: 100
  decoding_style: "unconstrained"
  base_model_name: "phi"
  temperature: 0.9
```

## Citation

```bibtex
@misc{williams2026beyondmarginal,
  title         = {Beyond Marginal Distributions: A Framework to Evaluate the Representativeness of Demographic-Aligned LLMs},
  author        = {Williams, Tristan and Weeber, Franzika and Padó, Sebastian and Akbik, Alan},
  eprint        = {2601.15755},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2601.15755}, 
  year          = {2026}
}
```
